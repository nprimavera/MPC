{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nprimavera/MPC/blob/main/Project3_V7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MECS6616 Spring 2025 - Project 3**"
      ],
      "metadata": {
        "id": "zo35PAC_CD9O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3yx4xOgKm_l"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- **Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/215046/pages/assignment-instructions) page on Courseworks to understand the workflow and submission requirements for this project.**\n",
        "\n",
        "**FOR PROJECT 3!!!**\n",
        "- Apart from the link to your notebook, you are also required to submit your collected data `.pkl` files and your chosen model checkpoint `.pth` files to Coursework. You will have two files each for parts 2 and 3 for a total of 4 files.\n",
        "- Your part 2 files should be named `data_pt2.pkl` and `dynamics_pt2.pth`.\n",
        "- Your part 3 files should be named `data_pt3.pkl` and `dynamics_pt3.pth`.\n",
        "- You should put the link to your notebook in the \"Comment\" section of your submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCleXWNaK8ef"
      },
      "source": [
        "## **Project Setup (do NOT change)**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- Do NOT change this \"*Project Setup*\" section\n",
        "- Do NOT install any other dependencies or a different version of an already provided package. You may, however, import other packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp25_project3' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp25_project3.git"
      ],
      "metadata": {
        "id": "V0KIvrmUtT8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83013c99-9db3-42d5-97cd-d1a2b8043770"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp25_project3'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 30 (delta 7), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (30/30), 49.76 KiB | 961.00 KiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# move all needed files into the working directory. This is simply to make accessing files easier\n",
        "!mv /content/mecs6616_sp25_project3/* /content/"
      ],
      "metadata": {
        "id": "KVOBvh6GvAgP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j8Cj2R4NyzSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44d64d3-74f2-4cd8-9650-5f433dad2701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-2.44.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray) (5.29.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.12.2)\n",
            "Downloading ray-2.44.0-cp311-cp311-manylinux2014_x86_64.whl (68.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.44.0\n"
          ]
        }
      ],
      "source": [
        "# Using ray for data collection is optional\n",
        "!pip install ray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIoNAwGQpHfH"
      },
      "source": [
        "# Starter Code Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project uses a simulated robot provided by the `Robot` class in `robot.py` file. Each robot is initialized with a corresponding forward dynamics (the base class for forward dynamics definition is in `arm_dynamic_base.py`). The arm_teacher is initialized with the provided ground truth forward dynamics, as defined in `arm_dynamics_teacher.py`. You are welcome to look in-depth into this file to understand how the ground truth forward dynamics is computed for an arm, given its number of links, link mass, and viscous friction of the environment - this is recommended but not necessary to successfully complete this assignment.\n",
        "\n",
        "The Robot class inside `robot.py` which provides the interface for controlling the robot arm i.e it provides you with some functions to set/get the state and set the action for the arm and take a step using the `Robot.advance()` method. The state of the arm is a 2n-dimensional vector: n joint positions [rad] + n joint velocities [rad/s] and the action is defined as the n torques (in N-m) applied to n joints respectively.\n",
        "\n",
        "In addition to `arm_dynamics_teacher.py` which contains the ground truth forward dynamics, you will use `arm_dynamics_student.py`, the student dynamics which internally uses a neural network model."
      ],
      "metadata": {
        "id": "gVWYjvF7wAs1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfEtTP6Gob6m"
      },
      "source": [
        "# Part 1: Implement Model Predictive Control"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "You will implement this controller by completing the MPC class. Specifically, you will implement the compute_action() method by following the algorithm discussed in the lecture. As with previous projects you are free to implement additional methods as needed or change the initialization if need be. While scoring your controller, you will be creating an instance of the MPC class and passing it to the scoring function so ensure that the arguments to the compute_action method remain the same.\n",
        "\n",
        "Although you do not need to understand how the ArmDynamicsTeacher class works, you could use the compute_fk() method from the class. This will allow you to convert from the state value (represented as array of shape (2*n, 1) where n is num_links) to final end effector position (x, y position of the end effector). Similary we can also compute the velocity of the end effector with the code below:\n",
        "```\n",
        "pos_ee = dynamics.compute_fk(state)\n",
        "vel_ee = dynamics.compute_vel_ee(state)\n",
        "```\n"
      ],
      "metadata": {
        "id": "hyjeF7RNvkgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "dgR69Xg9RsOT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Forward Models:  x_k+1 = f(x_k, u_k)\n",
        "    - x_k - state of the system at time step k\n",
        "    - u_k - action/command given to the robot at time step k\n",
        "    - x_k+1 - state of the system at the next time step, k+1\n",
        "    - using discretized time - size of time step Δt not explicitly part of forward model, but often very important\n",
        "    - Long Rollouts\n",
        "      - forward model that only predicts the next time step can still be used to look arbitrarily far into the future (if actions are given)\n",
        "      - errors quickly start compounding and the model diverges\n",
        "\n",
        "State - everything that \"characterizes\" your system\n",
        "\n",
        "Markovian property - state vector at current time step has everything you need to know about previous state vectors\n",
        "\n",
        "Kinematic Chain Dynamics:\n",
        "    - Torque-controlled dynamic robot arm\n",
        "        - x_k = (q, q_dot) - vectors of joint positions and velocities\n",
        "        - u_k = u - vector of applied joint torques\n",
        "        - x_k+1 = ?\n",
        "    - None of these are explicitly included in the state\n",
        "      - link masses\n",
        "      - inertial tensors\n",
        "      - externally applied forces (gravity)\n",
        "      - obstacles\n",
        "    - Transient contact makes analytical models of dynamics incredibly difficult\n",
        "      - linear law like F=ma no longer tells the full story since\n",
        "        - reaction forces are non-linear in relative object positions\n",
        "        - friction forces are non-linear in relative object velocity\n",
        "      - drastic changes (i.e. new contact) can occur in the middle of a time step\n",
        "        - must be handled in non-physical ways\n",
        "      - states that look very similar can be have drasticallty different\n",
        "\n",
        "Remember: analytical model of kinematic chains as ground truth\n",
        "    ● Step 1: solve for joint accelerations q\n",
        "      ○ Newton’s method: f = ma (for both forces and torques)\n",
        "      ○ Additional constraints ensure the arm is not coming apart\n",
        "      ○ Ends up as a big linear system of equations that you can solve for q\n",
        "    ● Step 2: numerical integration\n",
        "      ○ Exact:\n",
        "        ■ q_dot_k+1 = q_dot_k + ∫q_dot_dot(t)dt\n",
        "        ■ q_k+1 = q_k + ∫q_dot(t)dt\n",
        "      ○ Euler integration (linear!):\n",
        "        ■ q_dot_k+1 =q_dot_k +q_dot_dot_k * Δt\n",
        "        ■ q_k+1 = q_k + q_dot_k * Δt + 0.5 * q_dot_dot_k * Δt^2\n",
        "      ○ Much better schemes: Runge-Kutta, etc.\n",
        "\n",
        "Use your \"domain knowledge\" to inform the architecture of your model --> we know how dynamics work\n",
        "\n",
        "Overfit what the robot will need to do\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class MPC:\n",
        "\n",
        "    def __init__(self, dynamics):\n",
        "        self.control_horizon = 10  # Define control horizon\n",
        "        self.action = np.zeros((dynamics.get_action_dim(), 1))   # should be (3, 1)\n",
        "        self.num_links = dynamics.get_num_links()\n",
        "        self.best_cost = np.inf\n",
        "        self.best_action = np.zeros((self.num_links, 1))  # (3, 1) same as action shape\n",
        "        self.initial_state = np.array(state)\n",
        "        self.constant_step = 0.005  # remains constant\n",
        "        self.current_step = 0.005   # changes dynamically\n",
        "        self.position_weight = 2.07 # constant position weight\n",
        "        self.velocity_weight = 0.8  # constant velocity weight\n",
        "\n",
        "    def dynamic_step(self, distance_to_goal, current_velocity):\n",
        "        \"\"\"\n",
        "        Adjust step size based on distance & velocity dynamically\n",
        "          - start from 1\n",
        "          - add (0.5 * dist_to_goal) --> the farther the goal, the larger the adjustment\n",
        "          - add (0.5 * np.linalg.norm(current_velocity)) --> the higher the velocity, the larger the adjustment\n",
        "          - computes the new step size based on the sum of the two adjustments and ensures it doesn't go below 0.001\n",
        "        \"\"\"\n",
        "        #scale_factor = 1 + 0.5 * distance_to_goal + 0.5 * np.linalg.norm(current_velocity)\n",
        "        scale_factor = min(1 + 0.5 * distance_to_goal + 0.5 * np.linalg.norm(current_velocity), 10) # clipped to avoid very small step sizes\n",
        "        new_step = max(self.constant_step / scale_factor, 0.001)\n",
        "        return new_step\n",
        "\n",
        "    def dynamic_weight(self, distance_to_goal, current_velocity):\n",
        "        \"\"\"\n",
        "        Adjust position & velocity weights dynamically\n",
        "          - use a sigmoid function (returns b/w 0 and 1) to smoothly adjust position weight based on distance to goal\n",
        "          - (2 / (1 + np.exp(-distance_to_goal)) - 1) maps the weight adjustment between -1 and 1\n",
        "          - the further the goal, the higher the weight\n",
        "          - takes the magnitude (norm) of the current velocity\n",
        "          - caps the velocity at 1\n",
        "          - weight increases as the velocity increases\n",
        "        \"\"\"\n",
        "        updated_position_weight = self.position_weight * (2 / (1 + np.exp(-distance_to_goal)) - 1)\n",
        "        #updated_position_weight = self.position_weight * (1 + np.tanh(distance_to_goal))\n",
        "        updated_velocity_weight = self.velocity_weight * (1 + min(np.linalg.norm(current_velocity), 1))\n",
        "        return updated_position_weight, updated_velocity_weight\n",
        "\n",
        "    def arm_dynamics(self, dynamics):\n",
        "        # Available functions in arm_dynamics_teacher\n",
        "        print(\"\\nFunctions in dynamics_teacher:\")\n",
        "        print(dir(dynamics))\n",
        "        #print(dir(dynamics_teacher))\n",
        "\n",
        "        # Index of the force in the list of variables - each link has two forces\n",
        "        idx_f = dynamics.idx_f(0)\n",
        "        print(f\"\\nIndex of the force in the list of variables - each link has two forces (idx_f): {idx_f}\")\n",
        "\n",
        "        # Returns the index of acceleration for a link\n",
        "        idx_a = dynamics.idx_a(0)\n",
        "        print(f\"Returns the index of acceleration for a link (idx_a): {idx_a}\")\n",
        "\n",
        "        # Returns the index of angular acceleration for a link\n",
        "        idx_omdot = dynamics.idx_omdot(0)\n",
        "        print(f\"Index of angular acceleration for a link (idx_omdot): {idx_omdot}\")\n",
        "\n",
        "        # Total number of variables: position, velocity, angular velocity, forces, acceleration, torques\n",
        "        num_vars = dynamics.num_var()\n",
        "        print(f\"Total number of variables (num_vars): {num_vars}\")\n",
        "\n",
        "    def initial(self, dynamics, state, goal):\n",
        "        # Initial variables for action sequence\n",
        "        print(f\"\\nInitial action (should be zero): {self.action.reshape(-1)}\") # error handling\n",
        "        print(f\"Number of links: {self.num_links}\")\n",
        "        print(f\"Best cost (initially): {self.best_cost}\")\n",
        "        print(f\"Best action (initially): {self.best_action.reshape(-1)}\")\n",
        "\n",
        "        # Initial State\n",
        "        print(f\"Initial State: {self.initial_state.reshape(-1)}\")\n",
        "\n",
        "        # Initial End-Effector position and velocity\n",
        "        pos_ee = dynamics.compute_fk(self.initial_state)  # Get end effector position\n",
        "        vel_ee = dynamics.compute_vel_ee(self.initial_state)  # Get velocity of end effector\n",
        "        print(f\"Initial End-Effector position: {pos_ee.reshape(-1)}, Initial End-Effector velocity: {vel_ee.reshape(-1)}\")\n",
        "        print(f\"Goal: {goal.reshape(-1)}\")\n",
        "\n",
        "        # Initial deviation from Goal\n",
        "        distance_to_goal = np.linalg.norm(goal - pos_ee)  # Compute distance to goal\n",
        "        velocity_of_ee = np.linalg.norm(vel_ee)  # Compute velocity of end effector\n",
        "        print(f\"\\nInitial distance to goal: {distance_to_goal}, Initial velocity: {velocity_of_ee}\")\n",
        "\n",
        "    def forward_model(self, dynamics, state, goal, action):\n",
        "\n",
        "        # Current state\n",
        "        #self.state = state\n",
        "\n",
        "        # Forward simulation of next state to get the dynamics of the system using Euler integration method\n",
        "        next_state = dynamics.advance(state, action)  # predict next state using system dynamics\n",
        "        #next_state = dynamics.step(state, action)\n",
        "\n",
        "        # Compute position and velocity of end-effector at next state\n",
        "        pos_ee = dynamics.compute_fk(next_state)[:2, :]\n",
        "        vel_ee = dynamics.compute_vel_ee(next_state)[:2, :]\n",
        "\n",
        "        # Dynamically adjust steps and weights\n",
        "        distance_to_goal = np.linalg.norm(goal - pos_ee)\n",
        "        self.current_step = self.dynamic_step(distance_to_goal, vel_ee)\n",
        "        self.dynamic_position_weight, self.dynamic_velocity_weight = self.dynamic_weight(distance_to_goal, vel_ee)\n",
        "\n",
        "        return next_state, pos_ee, vel_ee\n",
        "\n",
        "    def cost_function(self, pos_ee, goal, vel_ee, action):\n",
        "\n",
        "        \"\"\"Quadratic Cost function: J = SUM_(i=k)^(N-1) α*||x_d - x_i||^2 + β*||u_i||^2\"\"\"\n",
        "\n",
        "        # Position error term:  α * ||x_d - x_i||^2\n",
        "        position_error = np.linalg.norm(pos_ee - goal) ** 2  # calculates the squared Euclidean distance b/w the current end-effector position and the goal\n",
        "        #position_error = np.sum((pos_ee - goal) ** 2)\n",
        "\n",
        "        # Velocity term:  β * ||u_i||^2\n",
        "        velocity_error = np.linalg.norm(vel_ee) ** 2  # calculates the squared norm of the control action\n",
        "        #velocity_error = np.sum(vel_ee ** 2)\n",
        "\n",
        "        # Cost function\n",
        "        #alpha=1.0 # position weight\n",
        "        #eta=0.1  # velocity weight\n",
        "        #cost = alpha * position_error + beta * control_effort\n",
        "        cost_function = position_error * self.dynamic_position_weight + velocity_error * self.dynamic_velocity_weight\n",
        "\n",
        "        #if cost_function < best_cost:\n",
        "            #best_cost = cost_function\n",
        "            #self.best_action = self.action.copy()\n",
        "        return cost_function\n",
        "\n",
        "    def compute_best_action(self, action, dynamics, goal, pos_ee, vel_ee):\n",
        "\n",
        "        # Compute how far the end-effector is from the goal\n",
        "        ee_dist_to_goal = pos_ee - goal  # vector\n",
        "        #ee_dist_to_goal = ee_dist_to_goal.flatten()  # flatten the vector into a 1D array\n",
        "\n",
        "        # Initialize new action vector\n",
        "        #new_action = self.action  # zeros to start - will store corrections based on ee_dist_to_goal\n",
        "        new_action = np.zeros((3, 1))\n",
        "\n",
        "        # Correct each robot link\n",
        "        #for i in range(action.shape[0]):\n",
        "        for i in range(self.num_links):\n",
        "            new_action[i, 0] = -ee_dist_to_goal[i % 2] * self.current_step\n",
        "\n",
        "        # Ensure `action` has the correct shape\n",
        "        action = np.reshape(action, (3, 1))  # Ensure action is (3,1)\n",
        "\n",
        "        if action.shape != new_action.shape:\n",
        "            raise ValueError(f\"Shapes of action {action.shape} and new_action {new_action.shape} do not match.\")\n",
        "\n",
        "        action += new_action\n",
        "        #self.action += new_action\n",
        "        #self.action = new_action\n",
        "\n",
        "        return action\n",
        "        #return self.action\n",
        "        #return new_action\n",
        "\n",
        "    def compute_action(self, dynamics, state, goal, action):\n",
        "        \"\"\"\n",
        "        Computes the optimal action using Model Predictive Control (MPC).\n",
        "            - dynamics: ArmDynamicsTeacher instance\n",
        "            - state: Current state of the system\n",
        "            - goal: Desired goal state\n",
        "            - action: Initial action guess\n",
        "            - return: Optimal action array of shape (num_links, 1)  -  (3, 1)\n",
        "\n",
        "        Forward Model Architecture:\n",
        "            - feed forward the state (position, velocity) and action into the MLP\n",
        "            - MLP outputs the acceleration\n",
        "            - use Euler integration to get you new state (new position, new velocity)\n",
        "            - compare to the ground truth (loss function)\n",
        "\n",
        "        Must return an array of shape (num_links, 1) - Action shape --> should be (3,1) since there are 3 links\n",
        "        \"\"\"\n",
        "\n",
        "        # Information about robot arm\n",
        "        #self.arm_dynamics(dynamics)  # prints values\n",
        "\n",
        "        # Initial values\n",
        "        #self.initial(dynamics, state, goal)  # prints values\n",
        "\n",
        "        # Initialize values for best action (3, 1)\n",
        "        #action = np.zeros((dynamics.get_action_dim(), 1))   # should be (3, 1)\n",
        "        best_action = np.zeros((self.num_links, 1))  # (3, 1) same as action shape\n",
        "        best_cost = np.inf  # Initialize best cost to infinity\n",
        "\n",
        "        # Initialize state to avoid overwriting input state\n",
        "        #updated_state = np.array(state, dtype=np.float64)\n",
        "        #print(f\"\\nUpdated state: {updated_state.reshape(-1)}\")\n",
        "\n",
        "        # Iterate through the control horizon - \"roll out\"\n",
        "        for _ in range(self.control_horizon):\n",
        "\n",
        "            # Solve for (1) joint accelerations and (2) Euler integration\n",
        "            #self.forward_model(dynamics, state, goal, action)\n",
        "            #next_state, pos_ee, vel_ee = self.forward_model(dynamics, updated_state, goal, action)  # forward model to predict next state\n",
        "            next_state, pos_ee, vel_ee = self.forward_model(dynamics, state, goal, action)  # forward model to predict next state\n",
        "            #print(f\"Next state: {next_state.reshape(-1)}\")\n",
        "            #print(f\"End-Effector position: {pos_ee.reshape(-1)}, End-Effector velocity: {vel_ee.reshape(-1)}\")\n",
        "\n",
        "            # Evaluate cost\n",
        "            #self.cost_function(state, goal, action)\n",
        "            cost = self.cost_function(pos_ee, goal, vel_ee, action)\n",
        "            #print(f\"Cost: {cost}\")\n",
        "\n",
        "            # Compute best action\n",
        "            if cost < best_cost:\n",
        "                best_cost = cost\n",
        "                best_action = action.copy()\n",
        "                #print(f\"Best cost: {best_cost}\")\n",
        "                #print(f\"Best action: {best_action.reshape(-1)}\")\n",
        "\n",
        "            # Compute new action\n",
        "            #self.compute_best_action(dynamics, goal)\n",
        "            #new_action = self.compute_best_action(dynamics, action, goal, pos_ee, vel_ee)\n",
        "            action = self.compute_best_action(action, dynamics, goal, pos_ee, vel_ee)\n",
        "            #print(f\"New action: {action.reshape(-1)}\")\n",
        "\n",
        "            # Update state for next iteration\n",
        "            #state = self.next_state\n",
        "            #updated_state = next_state\n",
        "            #print(f\"Updated state: {updated_state.reshape(-1)}\")\n",
        "            state = next_state\n",
        "\n",
        "            # Update action\n",
        "            #action = new_action\n",
        "            #print(f\"Updated action: {action.reshape(-1)}\")\n",
        "\n",
        "        return best_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br5wyFnGRxgg"
      },
      "source": [
        "## Manually testing the controller\n",
        "This part is for you to manually check the performance of your controller before you are ready for it be evaluated by our scoring function.\n",
        "To test your implementation run the following code. Feel free to play around with the cell or change the num_links / goal positions . You can define your controller however you would like to and then switch on gui to see how close your end effectors get to the goal position\n",
        "\n",
        "Every time step within the environment is 0.01s, which is defined in the dynamics as `dt`.\n",
        "\n",
        "The MPC class has a `control_horizon` variable which represents the frequency at which `controller.compute_action()` will be called\n",
        "\n",
        "In the scoring function you will be evaluated on the distance of your end effector to the goal position and the velocity of the end effector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bZ0GFCoLpJ9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1003bd66-3f51-47cb-b487-efdf3e614b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At timestep 0: Distance to goal: 3.3970575502926055, Velocity of end effector: 5.539171676143414e-18\n",
            "At timestep 10: Distance to goal: 3.392176332884339, Velocity of end effector: 0.06012181365351473\n",
            "At timestep 20: Distance to goal: 3.3780377788601825, Velocity of end effector: 0.11073440355184443\n",
            "At timestep 30: Distance to goal: 3.3530986691139173, Velocity of end effector: 0.14103106513380337\n",
            "At timestep 40: Distance to goal: 3.316499924736167, Velocity of end effector: 0.15250953873304238\n",
            "At timestep 50: Distance to goal: 3.268112537974774, Velocity of end effector: 0.14569628791585262\n",
            "At timestep 60: Distance to goal: 3.208694086680415, Velocity of end effector: 0.1209558794877226\n",
            "At timestep 70: Distance to goal: 3.1396013468962485, Velocity of end effector: 0.08071713201430115\n",
            "At timestep 80: Distance to goal: 3.062196297467657, Velocity of end effector: 0.02944534165538387\n",
            "At timestep 90: Distance to goal: 2.9773572187460595, Velocity of end effector: 0.02773566786991462\n",
            "At timestep 100: Distance to goal: 2.8853941058129005, Velocity of end effector: 0.08619807556095248\n",
            "At timestep 110: Distance to goal: 2.7862299089826843, Velocity of end effector: 0.1416355838554388\n",
            "At timestep 120: Distance to goal: 2.6797019722710136, Velocity of end effector: 0.19070180153581487\n",
            "At timestep 130: Distance to goal: 2.565896338830886, Velocity of end effector: 0.2311862955880504\n",
            "At timestep 140: Distance to goal: 2.4453852993664404, Velocity of end effector: 0.261880897841729\n",
            "At timestep 150: Distance to goal: 2.3193415735044693, Velocity of end effector: 0.2824214565369883\n",
            "At timestep 160: Distance to goal: 2.189599076345073, Velocity of end effector: 0.2932572778351789\n",
            "At timestep 170: Distance to goal: 2.058793578066686, Velocity of end effector: 0.2958384482909376\n",
            "At timestep 180: Distance to goal: 1.9307644341274985, Velocity of end effector: 0.29311455003442116\n",
            "At timestep 190: Distance to goal: 1.8114683081615677, Velocity of end effector: 0.29054122902124885\n",
            "At timestep 200: Distance to goal: 1.7107751709777193, Velocity of end effector: 0.29810954717648935\n",
            "At timestep 210: Distance to goal: 1.6456086411629225, Velocity of end effector: 0.33476765824369376\n",
            "At timestep 220: Distance to goal: 1.643596185691993, Velocity of end effector: 0.43367293385426114\n",
            "At timestep 230: Distance to goal: 1.7421412661148348, Velocity of end effector: 0.6427526549404425\n",
            "At timestep 240: Distance to goal: 1.9830668909356306, Velocity of end effector: 1.0384455175848109\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from arm_dynamics_teacher import ArmDynamicsTeacher\n",
        "from robot import Robot\n",
        "from render import Renderer\n",
        "from score import *\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Teacher arm with 3 links\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=3,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "\n",
        "arm = Robot(dynamics_teacher)\n",
        "arm.reset()\n",
        "\n",
        "#gui = True\n",
        "gui = False\n",
        "\n",
        "if gui:\n",
        "  renderer = Renderer()\n",
        "  time.sleep(1)\n",
        "\n",
        "# Controller\n",
        "controller = MPC(dynamics_teacher)\n",
        "#controller = MPC()\n",
        "\n",
        "# Resetting the arm will set its state so that it is in the vertical position,\n",
        "# and set the action to be zeros\n",
        "arm.reset()\n",
        "\n",
        "# Choose the goal position you would like to see the performance of your controller\n",
        "goal = np.zeros((2, 1))\n",
        "goal[0, 0] = 2.5\n",
        "goal[1, 0] = -0.7\n",
        "arm.goal = goal\n",
        "\n",
        "dt = 0.01\n",
        "time_limit = 2.5\n",
        "num_steps = round(time_limit/dt)\n",
        "\n",
        "# Initial action (start with zeros)\n",
        "action = np.zeros((arm.dynamics.get_action_dim(), 1))\n",
        "\n",
        "# Control loop\n",
        "for s in range(num_steps):\n",
        "  t = time.time()\n",
        "  arm.advance()\n",
        "\n",
        "  if gui:\n",
        "    renderer.plot([(arm, \"tab:blue\")])\n",
        "  time.sleep(max(0, dt - (time.time() - t)))\n",
        "\n",
        "  if s % controller.control_horizon==0:\n",
        "    state = arm.get_state()\n",
        "\n",
        "    # Measuring distance and velocity of end effector\n",
        "    pos_ee = dynamics_teacher.compute_fk(state)\n",
        "    dist = np.linalg.norm(goal-pos_ee)\n",
        "    vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))\n",
        "    print(f'At timestep {s}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')\n",
        "\n",
        "    action = controller.compute_action(arm.dynamics, state, goal, action)\n",
        "    arm.set_action(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVIGZZimCNI4"
      },
      "source": [
        "## Grading and Evaluation for Part 1\n",
        "Your controller will be graded on 6 tests. 2 tests each for the 1-link ,2-link, and 3-link arms. The arm will start off in the initial state with the arms pointing stright down. The testing criteria depend on the distance and the velocity of the end effectors . Each test will run the robot arm for **5.0 seconds**. At the end of the 5 seconds the test will be:\n",
        "\n",
        "A success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.1 and vel_ee < 0.5`\n",
        "\n",
        "A partial success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.2 and vel_ee < 0.5`\n",
        "\n",
        "After all of the tests, your score is summed up and then scaled out of 5 points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "IXswattzCLyC"
      },
      "outputs": [],
      "source": [
        "# Scoring using score_mpc\n",
        "controller = MPC(dynamics_teacher)\n",
        "gui = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "score_mpc_true_dynamics(controller, gui)"
      ],
      "metadata": {
        "id": "B5iMweqC1SsB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8081703-e232-4bf9-9f98-689f94f31fed"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 1 into shape (3,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-931e855729fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DO NOT CHANGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore_mpc_true_dynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/score.py\u001b[0m in \u001b[0;36mscore_mpc_true_dynamics\u001b[0;34m(controller, gui)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 result, pos_ee, vel_ee = test(arm, dynamics, goal, renderer, \\\n\u001b[0m\u001b[1;32m    297\u001b[0m                           controller, gui, args, dist_limit=[0.1, 0.2], time_limit=5.0)\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/score.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(arm, dynamics, goal, renderer, controller, gui, args, dist_limit, time_limit)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_horizon\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-3570e9602cec>\u001b[0m in \u001b[0;36mcompute_action\u001b[0;34m(self, dynamics, state, goal, action)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m#self.compute_best_action(dynamics, goal)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;31m#new_action = self.compute_best_action(dynamics, action, goal, pos_ee, vel_ee)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvel_ee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;31m#print(f\"New action: {action.reshape(-1)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-3570e9602cec>\u001b[0m in \u001b[0;36mcompute_best_action\u001b[0;34m(self, action, dynamics, goal, pos_ee, vel_ee)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Ensure `action` has the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure action is (3,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnew_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    297\u001b[0m            [5, 6]])\n\u001b[1;32m    298\u001b[0m     \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (3,1)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Learning Forward Dynamics"
      ],
      "metadata": {
        "id": "LsuMIhKPuNf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background Information\n",
        "\n",
        "From here on, you will only be working with a 2-link arm.\n",
        "\n",
        "In part 2, you will be training a model that learns the ground truth dynamics of a 2-link arm. This part uses two 2-link arms, one called arm_teacher (blue) and the other called arm_student (red), as shown in the image below. For each test, a torque will be applied to the first joint of both arms for 5 seconds. arm_teacher is moving according to the provided ground truth forward dynamics and your job is to use deep learning to train the arm_student to learn the forward dynamics of arm_teacher so that it can imitate its behavior. The forward dynamics is a function that takes in the current state of an action applied to the arm, and then computes the new state of the arm. This part uses a time step of 0.01 second, meaning each time we advance the simulation, we compute the forward dynamics for 0.01 second. In the example image, the student arm is not updating its state and remains static but we will make it move after training is done.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/roamlab/mecs6616_sp25_project3/blob/main/imgs/example.png?raw=true\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "The state of each arm is defined with a (4,1)-dimensional numpy array (two joint positions in radians + two joint velocities in radians per second). An action is defined as the two torques (in Nm) applied to the two joints respectively, which is a (2,1) numpy array. **Throughout this part, we make the problem simpler by only applying a torque to the first joint, so the actions always look like `[torque,0]`.** Also, when scoring your model the robot will always start off in a hanging position, meaning an initial state of `[-pi/2,0,0,0]` so if the collected data from part 2.1 looks similar, the model will perform better. The `robot.py` file provides you with some functions to set/get the state and set the action for the arm. Make sure you understand `robot.py` well enough before getting started.\n",
        "\n",
        "`geometry.py` provides some geometry functions, `render.py` defines how the visualization is rendered, `custom_plot.py` provides data visualization functionality. These three files are not of particular interest for completing this part."
      ],
      "metadata": {
        "id": "3UqbZ4e_zk54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.1: Collecting Data"
      ],
      "metadata": {
        "id": "zuthhNUKunUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will first need to complete the cell below to collect a dataset for training the forward dynamics. After running the cell, it should generate a pickle file `data_pt2.pkl` that contains a data dictionary `data = {'X': X, 'Y': Y}`. The shape of `data['X']` should be (`num_samples`, 6), the first 4 elements are state and the last 2 elements are the action. The shape of `data['Y']` should be (`num_samples`, 4), which saves the next state after applying the action using the ground truth forward dynamics of arm_teacher. You are responsible for deciding and collecting an appropriate amount of samples. Thus, num_samples is entirely up to you and your model.\n",
        "\n",
        "**After the data file is generated, `data_pt2.pkl` should appear under the 'Files' icon in the left sidebar. You can download this file by right clicking the file name. You are required to submit this file. Please do not change its name.**\n",
        "\n",
        "In the cell below, we have provided a minimal example of simulating the arm_teacher for 5 seconds. The GUI visualization is turned on and you should see the behavior of arm_teacher. The visualization can drastically slow down the simulator and you should turn it off when collecting a large amount of data."
      ],
      "metadata": {
        "id": "UR9W_fmSwdEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from arm_dynamics_teacher import ArmDynamicsTeacher\n",
        "from robot import Robot\n",
        "import pickle\n",
        "import math\n",
        "from render import Renderer\n",
        "import time\n",
        "from custom_plot import plot_positions_velocities_with_fixed_bounds\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# Teacher arm\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01\n",
        ")\n",
        "arm_teacher = Robot(dynamics_teacher)\n",
        "\n",
        "# ---\n",
        "# You code starts here. X and Y should eventually be populated with your collected data\n",
        "# Control the arm to collect a dataset for training the forward dynamics.\n",
        "X = np.zeros((0, arm_teacher.dynamics.get_state_dim() + arm_teacher.dynamics.get_action_dim()))\n",
        "Y = np.zeros((0, arm_teacher.dynamics.get_state_dim()))\n",
        "\n",
        "# We run the simulator for 5 seconds with a time step of 0.01 second,\n",
        "# so there are 500 steps in total\n",
        "num_steps = 500\n",
        "\n",
        "# GUI visualization, this will drastically reudce the speed of the simulator!\n",
        "# Set this to false once you understand how the code works\n",
        "gui = True\n",
        "\n",
        "# Define the initial state of the robot, such that it is vertical\n",
        "initial_state = np.zeros((arm_teacher.dynamics.get_state_dim(), 1))  # position and velocity\n",
        "initial_state[0] = -math.pi / 2.0\n",
        "\n",
        "# Set the initial state of the arm. Input to set_state() should be of shape (6, 1)\n",
        "arm_teacher.set_state(initial_state)\n",
        "\n",
        "# Define the action, applying 1Nm torque to the first joint\n",
        "action = np.zeros((arm_teacher.dynamics.get_action_dim(), 1))\n",
        "action[0] = 1\n",
        "\n",
        "# Set the action. Input to set_action() should be of shape (3, 1)\n",
        "arm_teacher.set_action(action)\n",
        "\n",
        "# Initialize the GUI\n",
        "if gui:\n",
        "    renderer = Renderer()\n",
        "    time.sleep(1)\n",
        "\n",
        "for s in range(num_steps):\n",
        "    # Get the current state\n",
        "    state = arm_teacher.get_state()\n",
        "\n",
        "    # The advance function will simulate the action for 1 time step\n",
        "    arm_teacher.advance()\n",
        "    if gui:\n",
        "        renderer.plot([(arm_teacher, 'tab:blue')])\n",
        "\n",
        "    # Get the new state after advancing one time step\n",
        "    new_state = arm_teacher.get_state()\n",
        "# ---\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# Save the collected data in the data_pt2.pkl file\n",
        "data = {'X': X, 'Y': Y}\n",
        "pickle.dump(data, open( \"data_pt2.pkl\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "UoH4UjAawlLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've stored X and Y in data, you may uncomment the cell below to see the distribution of end-effector positions and velocities"
      ],
      "metadata": {
        "id": "OEccRJTSwzSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of collected samples\n",
        "# plot_positions_velocities_with_fixed_bounds(data['Y'], [1,1])"
      ],
      "metadata": {
        "id": "pCxJmzR-wth8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.2: Learning Forward Dynamics"
      ],
      "metadata": {
        "id": "2Wc8mPVgw5-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "After the data is collected, you will then need to complete the cell below to use the collected dataset to learn the forward dynamics.\n",
        "\n",
        "The code already creates the dataset class and loads the dataset with a random 0.8/0.2 train/test split for you. This cell should save the model that it trains. You should use a specific procedure for saving, outlined below. Note that the saving code is not already included so you will have to add it yourself.\n",
        "\n",
        "In machine learning, it is a very good practice to save not only the final model but also the checkpoints, such that you have a wider range of models to choose from. We provide a code snippet for you and for each epoch of your training, you should use it to save the model at that epoch.\n",
        "\n",
        "```\n",
        "model_folder_name = f'epoch_{epoch:04d}_loss_{test_loss:.8f}'\n",
        "if not os.path.exists(os.path.join(model_dir, model_folder_name)):\n",
        "    os.makedirs(os.path.join(model_dir, model_folder_name))\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, model_folder_name, 'dynamics_pt2.pth'))\n",
        "```\n",
        "\n",
        "The output from running this code should be a folder as below:\n",
        "\n",
        "```\n",
        "models/\n",
        "    2025-03-08_23-57-50/\n",
        "        epoch_0001_loss_0.00032930/\n",
        "            dynamics_pt2.pth\n",
        "        epoch_0002_loss_0.00009413/\n",
        "            dynamics_pt2.pth   \n",
        "        ...  \n",
        "```\n",
        "\n",
        "You can see that every time you run this cell, a folder whose name is the time you started will be created under `models`. Checkpoints from all epochs will be saved and then the folder name for saving the checkpoint indicates the epoch number and loss on the holdout test set. Recording checkpoints this way allows you to easily pick the model with the smallest loss."
      ],
      "metadata": {
        "id": "6NqfsPwvw9e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important: choosing the best model\n",
        "\n",
        "Your code should keep track of the checkpoint with the smallest loss on the test set. You should save the path of that checkpoint to the variable `model_path`. An example value of `model_path` could be `models/2025-03-07_20-14-32/epoch_0046_loss_0.00000005/dynamics_pt2.pth`. In the evaluation code, the checkpoint from `model_path` will be loaded and evaluated.\n",
        "\n",
        "You should also download the `dynamics_pt2.pth` file to include in your submission.\n",
        "\n",
        "A common practice with regards to model saving is to only save models with the best test_loss, rather than checkpoint every epoch. If you want, you can modify the save code to do that instead."
      ],
      "metadata": {
        "id": "pNbmeC_Yxb0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture\n",
        "\n",
        "In lecture, we introduced the trick to use joint accelerations to compute the next state. You will use your neural network to compute the joint accelerations. This will take in your joint angles, joint velocities, and actions applied to the arm and output joint acceleration values for each link. Then you can use your knowledge of analytical kinematics to derive the joint positions and velocities in the next time step given the joint acceleration values."
      ],
      "metadata": {
        "id": "lSD5sprA6SNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import time\n",
        "import pickle\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "class DynamicDataset(Dataset):\n",
        "    def __init__(self, data_file):\n",
        "        data = pickle.load(open(data_file, \"rb\" ))\n",
        "        # X: (N, 6), Y: (N, 4)\n",
        "        self.X = data['X'].astype(np.float32)\n",
        "        self.Y = data['Y'].astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    # ---\n",
        "    # Your code goes here\n",
        "    pass\n",
        "    # ---\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    model.train()\n",
        "\n",
        "    # ---\n",
        "    # Your code goes here\n",
        "    # ---\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "\n",
        "    # --\n",
        "    # Your code goes here\n",
        "    test_loss = 0\n",
        "    # ---\n",
        "\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "# The ratio of the dataset used for testing\n",
        "split = 0.2\n",
        "\n",
        "# Do NOT change\n",
        "# We are only using CPU, and GPU is not allowed.\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "dataset = DynamicDataset('data_pt2.pkl')\n",
        "dataset_size = len(dataset)\n",
        "test_size = int(np.floor(split * dataset_size))\n",
        "train_size = dataset_size - test_size\n",
        "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, shuffle=True)\n",
        "\n",
        "# The name of the directory to save all the checkpoints\n",
        "timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model_dir = os.path.join('models', timestr)\n",
        "\n",
        "# Keep track of the checkpoint with the smallest test loss and save in model_path\n",
        "model_path = None\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(1, 1 + epochs):\n",
        "    # ---\n",
        "    # Your code goes here\n",
        "    # ---\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "vS589cRyxjNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction\n",
        "\n",
        "After you are done with training, you need to complete the cell below to load the saved checkpoint (in function init_model) and then use it to predict the new state given the current state and action (in function dynamics_step). Please do not modify the arguments to those functions, even though you might not use all of them."
      ],
      "metadata": {
        "id": "nnJolAUkx5iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from arm_dynamics_base import ArmDynamicsBase\n",
        "\n",
        "class ArmDynamicsStudent(ArmDynamicsBase):\n",
        "    def init_model(self, model_path, num_links, time_step, device):\n",
        "        # ---\n",
        "        # Your code hoes here\n",
        "        # Initialize the model loading the saved model from provided model_path\n",
        "        self.model = None\n",
        "        # ---\n",
        "        self.model_loaded = True\n",
        "\n",
        "    def dynamics_step(self, state, action, dt):\n",
        "        if self.model_loaded:\n",
        "            # ---\n",
        "            # Your code goes here\n",
        "            # Use the loaded model to predict new state given the current state and action\n",
        "            # Output should be an array of shape (4,1)\n",
        "            new_state = None\n",
        "            return new_state\n",
        "            # ---\n",
        "        else:\n",
        "            return state"
      ],
      "metadata": {
        "id": "w1G-VVcex-t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Grading\n",
        "\n",
        "The total number of points for this part is 5. There are 3 types of tests. Your grade for this part will be the average score across each type of test\n",
        "\n",
        "**For each type, there are 50 tests.** For each test, you get a score of 1, 0.5, or 0. Your final grade for each type is the averaged score across 50 tests * 5.\n",
        "\n",
        "- *Type 1*: for each test, a constant torque randomly sampled from [-1.5Nm, 1.5Nm] is applied to the first joint of the arm for 5 seconds. If the MSE (Mean Squred Error) between the predicted arm state (arm_student) and the ground truth arm state (arm_teacher) is < 0.0005, you get score 1 for this test. If 0.0005 <= MSE < 0.008, you get score 0.5 for this test. Otherwise you get 0.\n",
        "- *Type 2*: for each test, a torque that linearly increases from 0 to a random torque in [0.5Nm, 1.5Nm] is applied to the first joint of the arm for 5 seconds. If MSE < 0.0005, you get score 1 for this test. If 0.0005 <= MSE < 0.008, you get score 0.5 for this test. Otherwise you get 0.\n",
        "- *Type 3*: for each test, one torque is applied for the first 2.5 seconds and another torque is applied for the remaining 2.5 seconds. Both torques are sampled from [-1Nm, 1Nm]. If MSE < 0.015, you get score 1 for this test. If 0.015 <= MSE < 0.05, you get score 0.5 for this test. Otherwise you get 0.\n"
      ],
      "metadata": {
        "id": "T_zrr7POx_74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "# Set up grading\n",
        "\n",
        "\n",
        "import importlib\n",
        "import score\n",
        "importlib.reload(score)\n",
        "\n",
        "# Make sure model_path is correctly set\n",
        "print(model_path)\n",
        "\n",
        "# Create the teacher arm\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01\n",
        ")\n",
        "arm_teacher = Robot(dynamics_teacher)\n",
        "\n",
        "# Create the student arm\n",
        "dynamics_student = ArmDynamicsStudent(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01\n",
        ")\n",
        "if model_path is not None:\n",
        "  dynamics_student.init_model(model_path, num_links=2, time_step=0.01, device=torch.device('cpu'))\n",
        "arm_student = Robot(dynamics_student)"
      ],
      "metadata": {
        "id": "2YDlzP5EyF3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# Test on randomly sampled torques from [-1.5, 1.5]\n",
        "score.score_random_torque(arm_teacher, arm_student, gui=False)"
      ],
      "metadata": {
        "id": "HOd1d9pOyGjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# Test on torques that linearly increase from 0 to a random number from [0.5, 1.5]\n",
        "score.score_linear_torques(arm_teacher, arm_student, gui=False)"
      ],
      "metadata": {
        "id": "YdNGjOKnyI-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# Test on one torque applied to the first 2.5s and another torque applied to the second 2.5s\n",
        "# Both torques are sampled from [-1, 1]\n",
        "score.score_two_torques(arm_teacher, arm_student, gui=False)"
      ],
      "metadata": {
        "id": "TzFVWncbyKlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: MPC Controller with Learned Dynamics Model"
      ],
      "metadata": {
        "id": "I92M2SCUuxdV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O1cAx4rMce7"
      },
      "source": [
        "## Part 3.1: Model Architecture\n",
        "\n",
        "We have a base class Model and a subclass for the 2-link arm. The class Model is a base class for our models. In compute_next_state() method, you have to use the trick to use joint accelerations to compute the next state similar to what you did in Part 2.\n",
        "\n",
        "In the `Model2Link` class you will use a neural network to compute the joint accelerations by implementing `compute_qddot()` method. This will take 6 values (2 joint angles, 2 joint velocities and 2 actions applied to the arm) and output 2 joint acceleration values\n",
        "\n",
        "Do not change the arguments for the `__init__()` method even if you do not use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPOWJZOyMekB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "\tdef __init__(self, num_links, time_step):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.num_links = num_links\n",
        "\t\tself.time_step = time_step\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tqddot = self.compute_qddot(x)\n",
        "\t\tstate = x[:, :2*self.num_links]\n",
        "\t\tnext_state = self.compute_next_state(state, qddot)\n",
        "\t\treturn next_state\n",
        "\n",
        "\tdef compute_next_state(self, state, qddot):\n",
        "\n",
        "\t\t# Your code goes here\n",
        "\n",
        "\t\treturn None\n",
        "\n",
        "\tdef compute_qddot(self, x):\n",
        "\t\tpass\n",
        "\n",
        "class Model2Link(Model):\n",
        "  def __init__(self, time_step):\n",
        "    super().__init__(2, time_step)\n",
        "    # Your code goes here\n",
        "\n",
        "  def compute_qddot(self, x):\n",
        "    # Your code goes here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYr1nkuIUQpj"
      },
      "source": [
        "## Part 3.2: Collect Data\n",
        "Similar to part 2, we will collect data which will be used to learn a forward model for our 2 link robot arm. Once we have learnt a forward model you will be evaluated on your MPC Controller that uses the learnt dynamics model instead of the true dynamics.\n",
        "\n",
        "You can modify the collect_data function or write any of your own functions however you choose to. You will be evaluated on the **2 Link Robot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfli4gFgYA-_"
      },
      "outputs": [],
      "source": [
        "# Teacher arm with 2 links\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "\n",
        "arm = Robot(dynamics_teacher)\n",
        "arm.reset()\n",
        "\n",
        "def collect_data(arm):\n",
        "\n",
        "  # ---\n",
        "  # You code goes here. Replace the X, and Y by your collected data\n",
        "  # Control the arm to collect a dataset for training the forward dynamics.\n",
        "  num_samples = 10000\n",
        "  X = np.zeros((num_samples, arm.dynamics.get_state_dim() + arm.dynamics.get_action_dim()))\n",
        "  Y = np.zeros((num_samples, arm.dynamics.get_state_dim()))\n",
        "\n",
        "  # ---\n",
        "\n",
        "  return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Qtg48i9_LZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Call the function you have defined above to collect data\n",
        "X, Y = collect_data(arm)\n",
        "save_dir = 'dataset'\n",
        "if not os.path.exists(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "# Save the collected data in the data_pt3.pkl file\n",
        "data = {'X': X, 'Y': Y}\n",
        "pickle.dump(data, open(os.path.join(save_dir, 'data_pt3.pkl'), \"wb\" ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drBZgkLxf-O-"
      },
      "source": [
        "## Part 3.3: Training the forward model\n",
        "By now you would be familiar with the basic skeleton of training a forward model.\n",
        "\n",
        "The starter code already creates the dataset class and loads the dataset with a random 0.8/0.2 train/test split for you. This script should save the model that it trains. You should use a specific procedure for saving, outlined below.\n",
        "\n",
        "In machine learning, it is a very good practice to save not only the final model but also the checkpoints. Our starter code already configures save_dir for you and for each epoch of your training, you should use the following code to save the model at that epoch.\n",
        "\n",
        "```\n",
        "model_folder_name = f'epoch_{epoch:04d}_loss_{test_loss:.8f}'\n",
        "if not os.path.exists(os.path.join(args.save_dir, model_folder_name)):\n",
        "    os.makedirs(os.path.join(args.save_dir, model_folder_name))\n",
        "torch.save(model.state_dict(), os.path.join(args.save_dir, model_folder_name, 'dynamics_pt3.pth'))\n",
        "print(f'model saved to {os.path.join(args.save_dir, model_folder_name, \"dynamics_pt3.pth\")}\\n')\n",
        "```\n",
        "The output from running this code should be a folder as below:\n",
        "\n",
        "```\n",
        "models/\n",
        "    2021-03-24_23-57-50/\n",
        "        epoch_0001_loss_0.00032930/\n",
        "            dynamics_pt3.pth\n",
        "        epoch_0002_loss_0.00009413/\n",
        "            dynamics_pt3.pth   \n",
        "        ...  \n",
        "```\n",
        "You can implement the functions below as you please to collect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c1A-JN4gCtN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import pickle\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "class DynamicDataset(Dataset):\n",
        "  def __init__(self, datafile):\n",
        "    data = pickle.load(open(datafile, 'rb'))\n",
        "    # X: (N, 6), Y: (N, 4)\n",
        "    self.X = data['X'].astype(np.float32)\n",
        "    self.Y = data['Y'].astype(np.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "def train_one_epoch(model):\n",
        "  model.train()\n",
        "\t# ---\n",
        "\t# Your code goes here\n",
        "\t# ---\n",
        "  pass\n",
        "\n",
        "def test(model):\n",
        "\tmodel.eval()\n",
        "\t# --\n",
        "\t# Your code goes here\n",
        "  # --\n",
        "\ttest_loss = 0\n",
        "\treturn test_loss\n",
        "\n",
        "def train_forward_model():\n",
        "\n",
        "\t# --\n",
        "\t# Implement this function\n",
        "  # --\n",
        "\n",
        "  # Keep track of the checkpoint with the smallest test loss and save in model_path\n",
        "  model_path = None\n",
        "  max_test_loss = 1e4\n",
        "  model = Model2Link(0.01)\n",
        "\n",
        "  datafile = 'dataset/data_pt3.pkl'\n",
        "  split = 0.2\n",
        "  dataset = DynamicDataset(datafile)\n",
        "  dataset_size = len(dataset)\n",
        "  test_size = int(np.floor(split * dataset_size))\n",
        "  train_size = dataset_size - test_size\n",
        "  train_set, test_set = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)\n",
        "\n",
        "  # The name of the directory to save all the checkpoints\n",
        "  timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  model_dir = os.path.join('models', timestr)\n",
        "\n",
        "  epochs=50\n",
        "  for epoch in range(1, 1 + epochs):\n",
        "    # --\n",
        "    # Your code goes here\n",
        "    # --\n",
        "    pass\n",
        "\n",
        "  return model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4g5oanOA2lJ"
      },
      "outputs": [],
      "source": [
        "model_path = train_forward_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TWZSTyG15Es"
      },
      "source": [
        "## Part 3.4: Completing ArmDynamicsStudent\n",
        "\n",
        "After you are done with training, you need to complete ArmDynamicsStudent class following the comments below to load the saved checkpoint (in function init_model) and then use it to predict the new state given the current state and action (in function dynamics_step). Please do not modify the arguments to those functions, even though you might not use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc0s9sdV2hN9"
      },
      "outputs": [],
      "source": [
        "from arm_dynamics_base import ArmDynamicsBase\n",
        "\n",
        "class ArmDynamicsStudent(ArmDynamicsBase):\n",
        "    def init_model(self, model_path, num_links, time_step, device):\n",
        "        # ---\n",
        "        # Your code hoes here\n",
        "        # Initialize the model loading the saved model from provided model_path\n",
        "        self.model = None\n",
        "        # ---\n",
        "        self.model_loaded = True\n",
        "\n",
        "    def dynamics_step(self, state, action, dt):\n",
        "        if self.model_loaded:\n",
        "            # ---\n",
        "            # Your code goes here\n",
        "            # Use the loaded model to predict new state given the current state and action\n",
        "            new_state = None\n",
        "            return new_state\n",
        "            # ---\n",
        "        else:\n",
        "            return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK686tpd3jAU"
      },
      "source": [
        "## Manually Testing the MPC Controller with the learnt dynamics model\n",
        "We will now use the learnt dynamics model that you have trained. The model is loaded in the dynamics.init_model method. You can modify the goal positions to see how well is the controller performing similar to what you did before. Feel free to play around with the code in this cell to test your performance before the grading part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU54TdIv3JTG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from arm_dynamics_teacher import ArmDynamicsTeacher\n",
        "from robot import Robot\n",
        "from render import Renderer\n",
        "from score import *\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Teacher arm with 3 links\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "\n",
        "arm = Robot(dynamics_teacher)\n",
        "arm.reset()\n",
        "\n",
        "gui = False\n",
        "action = np.zeros((arm.dynamics.get_action_dim(), 1))\n",
        "if gui:\n",
        "  renderer = Renderer()\n",
        "  time.sleep(1)\n",
        "\n",
        "# Controller\n",
        "controller = MPC()\n",
        "dynamics_student = ArmDynamicsStudent(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# model_path should have the path to the best model that you have trained so far\n",
        "# which you would like to use for testing the controller\n",
        "model_path = None\n",
        "dynamics_student.init_model(model_path, 2, 0.01, device)\n",
        "\n",
        "# Control loop\n",
        "action = np.zeros((arm.dynamics.get_action_dim(), 1))\n",
        "goal = np.zeros((2, 1))\n",
        "goal[0, 0] = 2.7\n",
        "goal[1, 0] = 0.5\n",
        "arm.goal = goal\n",
        "\n",
        "dt = 0.01\n",
        "time_limit = 2.5\n",
        "num_steps = round(time_limit/dt)\n",
        "for s in range(num_steps):\n",
        "  t = time.time()\n",
        "  arm.advance()\n",
        "\n",
        "  if gui:\n",
        "    renderer.plot([(arm, \"tab:blue\")])\n",
        "  time.sleep(max(0, dt - (time.time() - t)))\n",
        "\n",
        "  if s % controller.control_horizon==0:\n",
        "    state = arm.get_state()\n",
        "    action = controller.compute_action(dynamics_student, state, goal, action)\n",
        "    arm.set_action(action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and Evaluation of Part 3\n",
        "You will be evaluated on how well your controller+learnt dynamics works together. The scoring functions consists of 16 random test goals all of which will be below the x axis and between 0.05 to 1.95 lengths away from the origin.\n",
        "The controller will call the compute_action method from your MPC class and apply the action for 10 timesteps\n",
        "```\n",
        "action = controller.compute_action(dynamics_student, state, goal, action)\n",
        "```\n",
        "\n",
        "Each test will run the robot arm for **2.5 seconds**. At the end of the 2.5 seconds the test will be:\n",
        "\n",
        "A success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.2 and vel_ee < 0.5`\n",
        "\n",
        "A partial success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.3 and vel_ee < 0.5`\n",
        "\n",
        "After all of the tests are complete, your score is summed up and then scaled out of 5 total points. You need 15 out of the 16 tests to succeed to get a full score."
      ],
      "metadata": {
        "id": "tn1RPPnLxrpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B28gYRyYEgrz"
      },
      "outputs": [],
      "source": [
        "controller = MPC()\n",
        "dynamics_student = ArmDynamicsStudent(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "model_path = 'dynamics_pt3.pth'\n",
        "gui=False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "score_mpc_learnt_dynamics(controller, dynamics_student, model_path, gui)"
      ],
      "metadata": {
        "id": "JgmrICx-1MGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5lIxv47dgEm"
      },
      "source": [
        "Time Limit\n",
        "*   MPC evaluation (Part 1): 30 minutes\n",
        "*   Data Collection with MPC (Part 2): 1 hour 20 minutes\n",
        "*   Training the forward dynamics model (Part 3): 40 minutes\n",
        "\n",
        "Note: Time limits are general guidelines. While your code must run within the allotted time, you should be able to achieve full score with a much shorter runtime\n",
        "\n",
        "\n",
        "Hints and Suggestions:\n",
        "1. You can use your MPC Controller in your data collection to gather better training samples\n",
        "2. A good cost function to evaluate your trajectory in MPC is very important and you can use both distance and velocity metrics to define the cost function.\n",
        "3. As mentioned in the lecture, a constant torque with pseudo gradients seems to work well for this project. You can also use multiple delta values to gather more trajectories to choose from.\n",
        "4. Since we are passing the MPC object to the controller you can instantiate the MPC class with different parameters like the planning horizon, delta values etc.\n",
        "5. To speed up data collection, avoid using np.concatenate(), np.stack() or np.append() like functions on your X and Y arrays. Instead, initialize X and Y arrays with all zeros using the correct shape and then fill in the values one by one. This is much faster in numpy. Using Ray is optional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Requirements and Hints\n",
        "\n",
        "Time Limit\n",
        "*   MPC evaluation (Part 1): 30 minutes\n",
        "*   Learned Forward Model (Part 2): Less than a hundred epochs of training (<= 25 mins) should suffice for achieving the full points. Again, the shorter your model training time is the better.\n",
        "*   Data Collection with MPC (Part 3): 1 hour 20 minutes\n",
        "*   Training the forward dynamics model (Part 3): 40 minutes\n",
        "\n",
        "Note: Time limits are general guidelines. While your code must run within the allotted time, you should be able to achieve full score with a much shorter runtime\n",
        "\n",
        "\n",
        "Hints and Suggestions:\n",
        "1. Choosing the right policy to collect datasets for this project (parts 2 and 3) is important. You need to think about how to do it properly so that your trained model will pass the tests successfully. It is in general very hard to learn the ground truth forward dynamics completely (that works for any distribution of actions), and during testing small errors can accumulate, leading to drastic failure in the end. You might want to try overfitting on the test cases to begin with.\n",
        "2. Make sure that your dataset for part 2 is less than 100 Mb, which is pretty much sufficient for achieving full marks. Collecting datasets can be time-consuming and you could parallelize this process for some speed-up using [ray](https://www.ray.io/). Make sure your data collection in part 2 takes <= 25 mins.\n",
        "3. You can use your MPC Controller in your data collection for part 3 to gather better training samples\n",
        "4. A good cost function to evaluate your trajectory in MPC is very important and you can use both distance and velocity metrics to define the cost function.\n",
        "5. As mentioned in the lecture, a constant torque with pseudo gradients seems to work well for this project. You can also use multiple delta values to gather more trajectories to choose from.\n",
        "6. Since we are passing the MPC object to the controller you can instantiate the MPC class with different parameters like the planning horizon, delta values etc.\n",
        "7. To speed up data collection, avoid using np.concatenate(), np.stack() or np.append() like functions on your X and Y arrays. Instead, initialize X and Y arrays with all zeros using the correct shape and then fill in the values one by one. This is much faster in numpy. Using Ray is optional\n",
        "8. You may reuse your part 2 model for part 3, but it may be beneficial to re-collect data to train a model specific to part 3.\n",
        "\n",
        "General Tips and Requirements\n",
        "- NO GPU: No GPU is required or allowed for this assignment and we will test your code without GPUs.\n",
        "- Loss Function: This is essentially a regression problem so think about what losses are suitable for regression.\n",
        "- Optimizer: While it is possible to use a simple optimizer to achieve the desired accuracy, the training time can be quite high. There exists a number of optimizers implemented in PyTorch that have much faster convergence.\n",
        "- Seeding. Please use seeding in your code to make sure your results are reproducible."
      ],
      "metadata": {
        "id": "laN6cP-xyetF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}